{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784812c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Befehle, die verschiedene Funktionen von Spacy laden. Basis für die Programmierung.\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59d61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Englisches Sprachpaket small und Spacytextblobpipe auf nlp laden\n",
    "nlp = spacy.load ('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "pathlist = Path(\"Songs\").glob('**/*.txt')\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eae2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion, die den Text normalisiert: Entfernt alle Satz- und Lehrzeichen, wandelt konjugierte Verben in Stammform um.\n",
    "# Gibt am Ende den normalisierten Text zurück. Funktion kann auf jedem beliebigen Text aufgerufen werden\n",
    "\n",
    "def normalize_ws (text):\n",
    "    norm_text = []\n",
    "    for token in text:\n",
    "            if not token.is_punct and not token.is_space:\n",
    "                    norm_text.append(token.lemma_.lower())\n",
    "    return ' '.join(norm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e71c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for path in pathlist:\n",
    "    #Textdatei wird eingelesen, Zeilenumbrüche werden entfernt\n",
    "    text = open(path).read()\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    \n",
    "    #Text wird in tokenisierten NLP Text umgewandelt\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #Path wird in String umgewandelt um den Dateinamen zuordnen zu können -funktioniert nicht\n",
    "    f_name, f_ext = os.path.splitext(path)\n",
    "    song_name = []\n",
    "    song_name.append(f_name)\n",
    "    \n",
    "    # Das \"DOC\" wird mit dem Funktionsaufruf \"normalize_ws\" normalisiert und abgespeichert\n",
    "    normalized_ws_text = normalize_ws(doc)\n",
    "    # Das normalisierte DOC muss wieder in einen NLP Text umgewandelt werden\n",
    "    normalized_ws_doc = nlp(normalized_ws_text)\n",
    "    # Alle Nouns werden aus dem normalisierten Text herausgefiltert:\n",
    "    nouns_ws = [ token.text for token in normalized_ws_doc if token.pos_ == 'NOUN']\n",
    "    # Alle Verben werden aus dem normalisierten Text herausgefiltert:\n",
    "    verbs_ws = [ token.text for token in normalized_ws_doc if token.pos_ == 'VERB']\n",
    "\n",
    "    #Anzahl der Token umgewandelt in String und dann in Token\n",
    "    number = len(nlp(normalized_ws_doc)) \n",
    "    number = str(number)\n",
    "    number = nlp(number)\n",
    "    \n",
    "    # Berechnung der 10 am häufigsten vorkommenden Nouns\n",
    "    ten_nouns = Counter(nouns_ws).most_common(10)\n",
    "    # Berechnung der 10 am häufigsten vorkommenden Verben\n",
    "    ten_verbs = Counter(verbs_ws).most_common(10)\n",
    "    #Arrays werden initialisiert\n",
    "    song_sent_score = []\n",
    "    song_sent_label = []\n",
    "    song_sent_subjectivity = []\n",
    "    total_pos = []\n",
    "    total_neg = []\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "\n",
    "    #Sentiment und Subjectivity wird dem Lied zugeordnet\n",
    "    sentiment = doc._.blob.polarity\n",
    "    sentiment = round(sentiment,2)\n",
    "    subjectivity = doc._.blob.subjectivity\n",
    "    subjectivity = round(subjectivity,2)\n",
    "\n",
    "    #Sentimentscore wird verwendet um das Lied als positives oder negatives einzustufen\n",
    "    if sentiment > 0:\n",
    "          sent_label = \"Positiv\"\n",
    "    else:\n",
    "      sent_label = \"Negativ\"\n",
    "\n",
    "    #Resultate werden den entsprechenden Kategorien zugeordnet\n",
    "    song_sent_label.append(sent_label)\n",
    "    song_sent_score.append(sentiment)\n",
    "    song_sent_subjectivity.append(subjectivity)\n",
    "\n",
    "    #Einzelne Tupel (Token + Sentiment) werden als positives oder negatives Wort geteilt\n",
    "    for x in doc._.blob.sentiment_assessments.assessments:\n",
    "      if x[1] > 0:\n",
    "        positive_words.append(x[0][0])\n",
    "      elif x[1] < 0:\n",
    "        negative_words.append(x[0][0])\n",
    "      else:\n",
    "        pass\n",
    "\n",
    "    #Resultate werden den entsprechenden Kategorien zugeordnet\n",
    "    total_pos.append(', '.join(set(positive_words)))\n",
    "    total_neg.append(', '.join(set(negative_words)))\n",
    "    \n",
    "    #Fehlende Arrays werden erstellt\n",
    "    total_nouns = []\n",
    "    total_verbs = []\n",
    "    total_amount = []\n",
    "\n",
    "    #Listen werden mit Panda als Basisdatei eingelesen\n",
    "    liste = pd.read_csv(\"liste.csv\")\n",
    "    liste1 = pd.read_csv(\"liste1.csv\")\n",
    "    liste2 = pd.read_csv(\"liste2.csv\")\n",
    "\n",
    "\n",
    "    #Liste wird mit Daten gefüllt\n",
    "    liste1 [\"Häufigste Nouns\"]= ten_nouns\n",
    "    liste2 [\"Häufigste Verben\"]= ten_verbs\n",
    "\n",
    "\n",
    "    #Liste 1 wird mit Daten gefüllt\n",
    "    liste[\"Dateiname\"]= song_name\n",
    "    liste[\"Anzahl aller Token\"] = number\n",
    "    liste[\"Sentiment Score\"] = song_sent_score\n",
    "    liste[\"Sentiment Label\"] = song_sent_label\n",
    "    liste[\"Sentiment Subjectivity\"] = song_sent_subjectivity\n",
    "    liste[\"Positive Wörter\"] = total_pos\n",
    "    liste[\"Negative Wörter\"] = total_neg\n",
    "    \n",
    "    strcount = str(count)\n",
    "\n",
    "    #Listen werden als CSV gespeichert\n",
    "    liste.to_csv(\"Sentiment\\sentiment\"+strcount+\".csv\")\n",
    "    liste1.to_csv(\"Nouns\\liste1\"+strcount+\".csv\")\n",
    "    liste2.to_csv(\"Verben\\liste2\"+strcount+\".csv\")\n",
    "    \n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4446fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe zur Kontrolle\n",
    "#print(nouns_ws)\n",
    "#print(verbs_ws)\n",
    "#print(words_ws)\n",
    "#print (doc)\n",
    "#print (normalized_ws_doc)\n",
    "#print(ten_nouns)\n",
    "#print(ten_verbs)\n",
    "#print(ten_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf16a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
